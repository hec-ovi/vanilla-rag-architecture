# ğŸš€ Vanilla RAG Architecture

A production-ready, local-first **Retrieval-Augmented Generation (RAG)** system that delivers state-of-the-art retrieval accuracy through semantic reranking. Built for developers who want AI-assisted workflows without cloud dependencies.

> **The Breakthrough**: Simple vector retrieval fused with cross-encoder reranking â€” the "secret sauce" that transforms basic RAG into a precision knowledge engine.

## âœ¨ Features

- **ğŸ” Hybrid Retrieval**: Vector search (top-10) + semantic reranking (top-3) for killer precision
- **ğŸ“„ Dynamic Ingestion**: Upload `.txt`, `.pdf`, `.png/.jpg` â€” processed and vectorized automatically
- **ğŸ–¼ï¸ Multimodal Ready**: Images captioned via vision models, treated as queryable documents
- **ğŸ¤– Local LLM Power**: Ollama integration with ROCm GPU acceleration (AMD Strix Halo optimized)
- **âš¡ Blazing Fast**: Flash Attention + Q8 KV cache quantization for efficient inference
- **ğŸ”’ Privacy-Locked**: 100% local â€” your data never leaves your machine
- **ğŸ¨ Sleek UI**: Vite + React frontend with drag-drop upload and real-time streaming

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Compose                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Frontend   â”‚    â”‚   Backend   â”‚    â”‚     Ollama      â”‚     â”‚
â”‚  â”‚  Vite+React â”‚â—„â”€â”€â–ºâ”‚   FastAPI   â”‚â—„â”€â”€â–ºâ”‚  LLM Inference  â”‚     â”‚
â”‚  â”‚   :3000     â”‚    â”‚    :8000    â”‚    â”‚    :11434       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                            â”‚                                    â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                     â”‚  Vector DB  â”‚                            â”‚
â”‚                     â”‚FAISS/Chroma â”‚                            â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Vite + React 19 + TypeScript |
| **Backend** | FastAPI + Python 3.12 + UV |
| **Embeddings** | SentenceTransformers (all-MiniLM-L6-v2) |
| **Vector DB** | FAISS (default) / Chroma (toggle) |
| **Reranker** | Cross-encoder (ms-marco-MiniLM-L-6-v2) |
| **LLM Engine** | Ollama (qwen2.5 or gpt-oss) |
| **Vision** | Ollama Vision (llava/qwen2.5-vl) |
| **GPU** | ROCm (AMD Strix Halo gfx1151) |

## ğŸš€ Quick Start

### Prerequisites

- Ubuntu 25.10+ (Kernel 6.17+)
- Docker with Compose plugin
- AMD GPU with ROCm support (or CPU fallback)

### 1. Clone & Configure

```bash
git clone <repo-url>
cd vanilla-rag-architecture

# Copy and edit environment configuration
cp .env.template .env
# Edit .env with your host paths
```

### 2. Prepare Directories

```bash
# Create data directories (update paths to match your .env)
mkdir -p /home/$USER/models/ollama
mkdir -p /home/$USER/vanilla-rag/data
```

### 3. Launch

```bash
docker compose up -d --build
```

### 4. Verify

```bash
# Check all services are healthy
docker compose ps

# Test Ollama GPU inference
curl -s http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:14b",
  "prompt": "What is 2+2?",
  "stream": false
}' | jq .

# Test backend
curl http://localhost:8000/health

# Open frontend
open http://localhost:3000
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ docker-compose.yml          # Root orchestration
â”œâ”€â”€ .env.template               # Configuration template
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ .gitignore                  # Comprehensive ignore rules
â”œâ”€â”€ backend/                    # FastAPI application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ frontend/                   # Vite React application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx
â”‚       â””â”€â”€ ...
â””â”€â”€ ollama/                     # ROCm GPU inference service
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ entrypoint.sh
```

## ğŸ”§ Configuration

Key environment variables (see `.env.template` for all options):

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_MODELS_DIR` | â€” | Host path for model storage |
| `DATA_DIR` | â€” | Host path for vector DB and uploads |
| `OLLAMA_MODEL` | `qwen2.5:14b` | LLM model to use |
| `OLLAMA_CONTEXT_LENGTH` | `8192` | Context window size |
| `VECTOR_DB_TYPE` | `faiss` | Vector DB: `faiss` or `chroma` |
| `CHUNK_SIZE` | `500` | Text chunk size |
| `CHUNK_OVERLAP` | `100` | Chunk overlap |
| `TOP_K_RETRIEVE` | `10` | Initial retrieval count |
| `TOP_K_RERANK` | `3` | Final reranked results |

## ğŸ¯ RAG Pipeline

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Search   â”‚â”€â”€â–º Top 10 candidates
â”‚ (Embeddings)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cross-Encoder   â”‚â”€â”€â–º Top 3 reranked
â”‚ Reranking       â”‚    (Semantic precision)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Generation  â”‚â”€â”€â–º Response + Sources
â”‚ (Context-aware) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing RAG

The system includes test data to prove the RAG power:

1. Upload `rag_techniques_test.txt` (auto-generated on first run)
2. Ask obscure questions like: *"What is HyPE vs HyDE in RAG?"*
3. Watch as the system retrieves and answers from context â€” something base LLMs can't do

## ğŸ› ï¸ Development

### Backend Only

```bash
cd backend
pip install -r requirements.txt
uvicorn src.main:app --reload --port 8000
```

### Frontend Only

```bash
cd frontend
npm install
npm run dev
```

## ğŸ“œ License

MIT License â€” build, modify, deploy freely.

---

**Built with â¤ï¸ for the local AI revolution.**

*If component chaos is killing your workflow, this is your game-changer.*

#RAG #LocalAI #AMD #ROCm #FastAPI #React
